\documentclass{superfri}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{multirow}
\usepackage{amsmath}
\usepackage{babel}
\usepackage[official]{eurosym}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[list=true,hypcap=true]{subcaption}
\usepackage{units}
\usepackage{xfrac}
\usepackage{color}

\usepackage{varioref}
\usepackage[hidelinks]{hyperref}
\usepackage[capitalise,noabbrev]{cleveref} % Version 0.18.10

\bibliographystyle{plain}
\numberwithin{equation}{section}


\begin{document}

% PAGE LIMIT: full paper 10-16 pages

\author{Julian M. Kunkel\footnote{\label{dkrz}Deutsches Klimarechenzentrum (DKRZ), Hamburg, Germany}, 
Jay Lofstead\footnote{Center for Computing Research, Sandia National Laboratories, Albuquerque, USA}, 
Colin McMurtrie\footnote{Swiss National Computing Center (CSCS), Lugano, Switzerland}, 
PLEASE ADD YOURSELF
} % \footnoteref{dkrz}

\title{Data Center Perspectives on HPC-IO}
\maketitle{}

\begin{abstract}

\noindent
\keywords{Parallel I/O, data center, file systems, managing data}
\end{abstract}

% -----------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}

Many public and private funded data centers host supercomputers for running large scale simulations and analysis of experimental and observational data.
Those supercomputers run usually tightly coupled parallel applications that require hardware components that deliver the best performance.
In contrast, commercial data centers such as Facebook and Google execute loosely coupled workloads and can tolerate failures much easier.
The dimension of the data centers is enormous.
In 2013, an article was published that summarizes commercial data centers' dimensions\,\cite{data13}, it estimates, for example, that 
Facebook hosts around 100\,PB of storage and Google and Microsoft manage around 1 Million servers each -- although the hardware is split among several physical data centers -- a modus operandi not suitable for HPC centers.
With the hunger for information, the globally installed storage capacity increases exponentially and is expected to hit 7,235 exabytes by 2017\,\cite{EXA13}.
This trend can also be seen in the sale reports of companies such as the disk drive manufactorer Seagate; within 5 years they shipped 1 billion HDDs, which means 700.000 units every day\,\cite{SG14}.  
With state-of-the-art 8\,TB disks, this would already account for 5.5 exabyte of capacity by day.

Management of the huge amount of data is vital for effective use of the contained information, however, with limited budget, it is a daunting task for data centers.
Especially as design and hardware required for the storage system depends heavily on the executed workloads.
A co-factor of the increasing difficulty is the increase in complexity of the storage hierachy with the adoption of SSD and memory class storage technology.
The department of energy recognizes the importance of data management, listing it among the top 10 research challenges for Exascale\,\cite{top14}. % the document says: “Affordability of data management, both procurement and operational, is a huge challenge.“

There are several initiatives, consortia and special tracks in conferences that target RD\&E audiences.
Examples are the Storage Networking Industry Association (SNIA) for enterprises, the 
Big Data and Extreme-Scale Computing (BDEC) initiative\footnote{\url{http://www.exascale.org/bdec/}}, the 
Exascale10 workgroup\,\cite{brinkmann14}, the Parallel Data Storage Workshop (PDSW), the HEC FSIO workshop \cite{bancroft2009} and the HPC-IODC workshop\,\cite{iodc}.

There are also many studies of I/O workloads and performance analysis reports for parallel I/O available.
Additionally, many surveys of usage of enterprise technology are made including predictions of analysis for future storage technology and the storage market such as\,\cite{idc1}.
However, analysis conducted for HPC typically focus on applications and not on the data center perspective.
Information about operational aspects of data centers is usually described in file system specific user groups and meetings or described partially in research papers as part of the evaluation environment.

% In \cite{luu2015multiplatform}, logs automatically captured with the Darshan tool are analyzed.
%Studies: Software-Defined storage
%\cite{http://datacore.com/sf-docs/default-source/whitepapers/english/the-state-of-sds-2015-survey.pdf}
%cloud storage
%\cite{http://www.ctera.com/enterprise-cloud-storage-survey-2015}
%Siehe auch http://www.prnewswire.com/news-releases/2015-ctera-enterprise-cloud-storage-survey-highlights-data-governance-and-security-concerns-continuing-preference-for-private-and-virtual-private-clouds-300092189.html

In this paper, we present a contemporary overview of several data centers that participated in the HPC-IODC workshop 2015.
The centers provide a system perspective about storage architectures, strategies for data mangement and production issues but also conducted RD\& to overcome the current obstacles.

\section{Participating Data Centers}
\label{sec:centers}

\textbf{We'll use this section as brief advertisement for all data centers and include a short description about key workloads.}

An overview of the characteristics of the data centers is given in \Cref{tbl:overviewCharacteristics}.
The I/O systems of the current supercomputers are characterized in \Cref{tbl:overviewIO}.


\paragraph{DKRZ}
Typical workloads.

\paragraph{CSCS}

\paragraph{HLRS}
Hornet


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[bt]
\renewcommand{\arraystretch}{0.8}
\renewcommand{\tabcolsep}{0.1cm}
\begin{tabular}[c]{ll|r|r|r|r||r||r|r}
Center & Top    & \multicolumn{4}{c||}{Compute}      & \multicolumn{1}{c||}{Network}               & \multicolumn{2}{c}{Archive} \\
 & system & Peak perf. & Node cnt & Cores/Node & Total mem & Type                 & Tape slots & Peak perf. \\ \hline
 \hline

DKRZ & Mistral & 1.49\,PF   & 1500       & 24         & IB FDR                     & 65,000 & XY\,MiB/s \\ \hline
Sandia & Sky Brige & 615\,TF   & Intel Truscale \\ \hline
CSCS & Piz Daint & 7.79\,PF & Cray Aries \\ \hline
HLRS & Hornet & 3.79\,PF & 3944 & 24 & 493 TB & Cray Aries &   \\ \hline
% CSCS & 

\end{tabular}
\caption{Data center system characteristics\label{tbl:overviewCharacteristics}}
\end{table}

\begin{table}[bt]
\renewcommand{\arraystretch}{0.8}
\renewcommand{\tabcolsep}{0.1cm}
\begin{tabular}[c]{l|r|r|r|r|r|r|r}
Center & Type        & Capacity  & Peak perf.     & Server cnt & File systems & File count    & Avg file size    \\ \hline
DKRZ   & Lustre\,2.5 & 20\,PB    & 313\,GiB/s     & 60         & 1            &               &                  \\ \hline
HLRS   & Lustre  & 8.1\,PB       & 150\,GiB/s     & 112        & 7            & 3.5\,M        & 
% CSCS &

\end{tabular}
\caption{Data center storage characteristics\label{tbl:overviewIO}}
\end{table}

\section{Managing Storage}

To utilize storage efficiently, data centers have to decide concepts and policies for the data lifecycle of users.
This includes, for example, quota restrictions and the partioning of the available hardware into usable file systems.

DKRZ: Split into Scratch, Work, Projects. One Pool for Lustre. DNE-phase 1 distributes the top level directories among the metadata servers.

HLRS: Work Space mechanism
• A directory in the project file system is created upon request
with a user defined name
• The directory is available for 30 days
• The directory life time can be extended 3 times by 30 days
• At the end of life, the directory with its content!!! is
automatically deleted
• There are tools for
– finding available workspaces
– Releasing workspaces
– Setting a reminder in calender tools
• Quota is enabled


\section{Monitoring}
\label{sec:monitoring}

The monitoring of a shared resource such as a file system is vital to ensure a fair distribution of resources and to identify bottlenecks in hardware and software.



\section{Production Issues}
\label{sec:issues}

Lustre:
- DNE issues
- High-level APIs such as NetCDF and HDF5 perform suboptimal, roughly 10\% of peak.
- Quota
- Full OSTs

\section{Conducted R\&D}
\label{sec:randd}

Investigation of alternative storage e.g. for workloads with random-access characteristics.

\section{Summary, Conclusions}
\label{sec:summary}


\ack{%
\noindent
Acknowledgment:}

\bibliography{literature}


\end{document}
