\documentclass{llncs}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[official]{eurosym}
\usepackage{graphicx}
\usepackage{color}

\usepackage[hidelinks]{hyperref}
\usepackage[capitalise,noabbrev]{cleveref} % Version 0.18.10


\begin{document}
\mainmatter


\author{Julian M. Kunkel, Jay Lofstead, Colin McMurtrie
} 

\title{HPC I/O in the Data Center Workshop (HPC-IODC)}

\institute{
	Deutsches Klimarechenzentrum\\ 	Bundesstraße 45a\\	20146 Hamburg, Germany\\ \email{kunkel@dkrz.de} 	
	\and
	Center for Computing Research\\ Sandia National Laboratories\\ Albuquerque, USA
	\and
	Swiss National Computing Center (CSCS) \\ Lugano, Switzerland
}

\maketitle{}

%\begin{abstract}
%This is the preface 
%\noindent
%\keywords{Parallel I/O, data center, file systems, managing data}
%\end{abstract}

% -----------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}

Many public and privately funded data centers host supercomputers for running large scale simulations and analyzing experimental and observational data.
These supercomputers run usually tightly coupled parallel applications that require hardware components that deliver the best performance.
In contrast, commercial data centers, such as Facebook and Google, execute loosely coupled workloads with a broad assumption of regular failures.
The dimension of the data centers is enormous.
A 2013 article summarizes commercial data centers' dimensions\,\cite{data13}. It estimates, for example, that 
Facebook hosts around 100\,PB of storage and Google and Microsoft manage around 1 million servers each -- although the hardware is split among several physical data centers -- a modus operandi not suitable for HPC centers.
With the hunger for information, the globally installed storage capacity increases exponentially and is expected to hit 7,235 exabytes by 2017\,\cite{EXA13}.
This trend is visible in the sales reports of companies such as the disk drive manufacturer Seagate. Within 5 years, they shipped 1 billion HDDs, which means 700.000 units every day\,\cite{SG14}.  
With state-of-the-art 8\,TB disks, this would already account for 5.5 exabyte of capacity by day.

Management of the huge amount of data is vital for effective use of the contained information. However, with limited budgets, it is a daunting task for data center operators,
especially as design and storage system required hardware depends heavily on the executed workloads.
A co-factor of the increasing difficulty is the increase in complexity of the storage hierarchy with the adoption of SSD and memory class storage technology.
The US Department of Energy recognizes the importance of data management, listing it among the top 10 research challenges for Exascale\,\cite{top14}. % the document says: “Affordability of data management, both procurement and operational, is a huge challenge.“

There are several initiatives, consortia and special tracks in conferences that target RD\&E audiences.
Examples are the Storage Networking Industry Association (SNIA) for enterprises, the 
Big Data and Extreme-Scale Computing (BDEC) initiative\footnote{\url{http://www.exascale.org/bdec/}}, the 
Exascale10 workgroup\,\cite{brinkmann14}, the Parallel Data Storage Workshop (PDSW) and the HEC FSIO workshop \cite{bancroft2009}.

There are many I/O workloads studies and performance analysis reports for parallel I/O available.
Additionally, many surveys of enterprise technology usage include predictions of analysis for future storage technology and the storage market such as\,\cite{idc1}.
However, analysis conducted for HPC typically focuses on applications and not on the data center perspective.
Information about data center operational aspects is usually described in file system specific user groups and meetings or described partially in research papers as part of the evaluation environment.

In this workshop, we bring together I/O experts from data centers and application workflows to share current practices for scientific workflows, issues and obstacles for both hardware and the software stack, and R\&D to overcome these issues. 


\section{Organization of the Workshop}

\noindent The workshop content was built on three tracks:
\begin{itemize}
  \item \textbf{Research paper presentations} -- authors needed to submit a paper regarding relevant research for I/O in the datacenter.
  \item \textbf{Talks from I/O experts} -- authors needed to submit a rough outline for the talk related to the operational aspects of the data center.
  \item \textbf{Invited track} for a keynote and two moderated discussion slots.
\end{itemize}        
    
\noindent The CFP has been issued beginning of January.
Important deadlines were:
\begin{itemize}
  \item Submission deadline: 28-02-2016 AoE
  \item Author notification: 23-03-2016
  \item Workshop: 23-05-2016
  \item Camera-ready papers: 23-06-2016  
\end{itemize}

From all submissions, the programm committee selected four talks from I/O experts and four research papers for presentation during the workshop.

\subsection{Programm Committee}
\begin{itemize}
  \item Wolfgang Frings (\textit{Jülich Supercomputing Center, Germany})
  \item Javier Garcia Blas (\textit{University Carlos III of Madrid, Spain})
  \item  Rob Ross (\textit{Argonne National Laboratory, USA})
  \item   Carlos Maltzahn (\textit{University of California, Santa Cruz, USA})
  \item  Kathryn Mohror (\textit{Lawrence Livermore National Laboratory, USA})
  \item  Xiaosong Ma (\textit{North Carolina State University, Oak Ridge National Laboratory, USA})
  \item  Julian Kunkel (\textit{DKRZ, Germany})
  \item  Jay Lofstead (\textit{Sandia National Laboratory, USA})
  \item  Colin McMurtrie (\textit{CSCS, Switzerland})
\end{itemize}



\section{Workshop Summary}
\label{sec:summary}

Throughout the day, on average 33 participants attended the workshop.
We had a good mix of talks from I/O experts, data center relevant research and also discussions.
A short summary of the presentations is given in the following.
The slides of the presentations are available on the workshop's webpage: \\ 
\url{http://wr.informatik.uni-hamburg.de/events/2016/iodc}.

The keynote from Rob Ross advocated the use of tools to study I/O actities from the data center perspective.
He presented results from I/O monitoring using Darshan, that point out the benefit of centralized monitoring.
Also, he presented some studies with the I/O simulation tool Codes for understanding object storage and the dragon fly network topology.
Finally, the recently started Tokio project is introduced that will lift I/O monitoring to a next level.

\subsection{Research Papers}

In the research paper “Delta: Data Reduction for Integrated Application Workflows and Data Storage”, the ADIOS middleware is extended with a compression method.
It compresses a time series of data with delta encoding by comparing data to the initial values.
A result of the study is that already this approach reduces the data volume for in-situ analysis significantly for applications.

In the second research paper “The Effect of Python and NetCDF on the Read Performance when using HPC Parallel Filesystems”, evaluates various aspects of NetCDF performance on several storage systems.
It is shown that there are some issues in the Python libraries which reduce performance significantly.

Next, in the paper “Analyzing Data Properties using Statistical Sampling Techniques“, a method is presented to evaluate data characteristics such as scientific file formats and compression ratio on a subset of data to estimate the value for the full system.
It turns out, that a small subset of data is sufficient to predict the true value of characteristics that are computed by file numbers or storage capacity accurately.

The last paper, “An Overview of the Sirocco Parallel Storage System”, introduces the Sirocco distributed object storage.
In contrast to parallel file systems, the consistency model is relaxed and there is a-priori no explicit central index for the data. This allows writers to proceed independetly and even adjust their storage targets based on system utilization.

\subsection{Talks from Experts}

The four talks from experts included information about the site and typical application profiles but also contained information regarding I/O tools and strategies. 
In the first talk, we heard about LRZ's storage system and strategy.
Utilizing file system monitoring and the Persyst tool an overview of the bottleneck for an application is gained.
Additionally, Darshan, Scalasca and VampirTrace is used for I/O characterisation and analysis.
An emphasis was made to present and demonstrate a methodology to identify I/O bottlenecks that is applied in the data center with success.

Then, we heard an update of HLRS data management plans.
It was demonstrated, that the storage system's peak performance is barely utilized from typical applications.
Additionally, potential strategies and architectures for a future system has been sketched relying on NVRAM.

The next talk described I/O monitoring at Jülich Supercomputing Center.
The LLview tool provides a good overview of ongoing jobs but also a history. 
This includes system statistics and I/O activity and can be used to analyze the overall system behavior and utilization.
Next, SIONlib has been introduced, which manages shared files for task-local data.
Finally, the activities in the DEEP-ER project where described, in which SIONlib manages buddy checkpoints.

Our final talk covered CSCS storage system and ...
TODO: Colin....


\subsection{Discussion rounds}


The first discussion starter is based on the community effort of the Virtual Institute for I/O (VI4IO)~\footnote{\url{http://vi4io.org}}.
VI4IO aims to provide a community hub containing research groups, relevant tools to monitor and benchmark HPC storage behavior, events. 
Finally it hosts and manages the High-Performance Storage List (HPSL), which currently contains 30 high-performance storage systems including their characteristics.
In contrast to existing lists, operators of a data center can create and manage the list themselves and can provide additional prosa text describing site, system and storage.
Since it is very difficult to find detailed information about existing storage systems, there are still a lot of characteristics missing, still the first analysis of storage capacity have been made and presented during the talk.
The discussion with the attendees revealed optimization potential in the presentation of the characteristics that are already resolved in the current list.

TODO: Benchmarking / NVRAM (Jay)...



\bibliographystyle{splncs}
\bibliography{literature}


\end{document}
