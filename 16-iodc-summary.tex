\documentclass{llncs}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[official]{eurosym}
\usepackage{graphicx}
\usepackage{color}

\usepackage[hidelinks]{hyperref}
\usepackage[capitalise,noabbrev]{cleveref} % Version 0.18.10


\begin{document}
\mainmatter


\author{Julian M. Kunkel, Jay Lofstead, Colin McMurtrie
} 

\title{HPC I/O in the Data Center Workshop (HPC-IODC)}

\institute{
	Deutsches Klimarechenzentrum\\ 	Bundesstraße 45a\\	20146 Hamburg, Germany\\ \email{kunkel@dkrz.de} 	
	\and
	Center for Computing Research\\ Sandia National Laboratories\\ Albuquerque, USA
	\and
	Swiss National Computing Center (CSCS) \\ Lugano, Switzerland
}

\maketitle{}

%\begin{abstract}
%This is the preface 
%\noindent
%\keywords{Parallel I/O, data center, file systems, managing data}
%\end{abstract}

% -----------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}

Many public and privately funded data centers host supercomputers for running large scale simulations and analyzing experimental and observational data.
These supercomputers run usually tightly coupled parallel applications that require hardware components that deliver the best performance.
In contrast, commercial data centers, such as Facebook and Google, execute loosely coupled workloads with a broad assumption of regular failures.
The dimension of the data centers is enormous.
A 2013 article summarizes commercial data centers' dimensions\,\cite{data13}. It estimates, for example, that 
Facebook hosts around 100\,PB of storage and Google and Microsoft manage around 1 million servers each -- although the hardware is split among several physical data centers -- a modus operandi not suitable for HPC centers.
With the hunger for information, the globally installed storage capacity increases exponentially and is expected to hit 7,235 exabytes by 2017\,\cite{EXA13}.
This trend is visible in the sales reports of companies such as the disk drive manufacturer Seagate. Within 5 years, they shipped 1 billion HDDs, which means 700.000 units every day\,\cite{SG14}.  
With state-of-the-art 8\,TB disks, this would already account for 5.5 exabyte of capacity by day.

Management of the huge amount of data is vital for effective use of the contained information. However, with limited budgets, it is a daunting task for data center operators,
especially as design and storage system required hardware depends heavily on the executed workloads.
A co-factor of the increasing difficulty is the increase in complexity of the storage hierarchy with the adoption of SSD and memory class storage technology.
The US Department of Energy recognizes the importance of data management, listing it among the top 10 research challenges for Exascale\,\cite{top14}. % the document says: “Affordability of data management, both procurement and operational, is a huge challenge.“

There are several initiatives, consortia and special tracks in conferences that target RD\&E audiences.
Examples are the Storage Networking Industry Association (SNIA) for enterprises, the 
Big Data and Extreme-Scale Computing (BDEC) initiative\footnote{\url{http://www.exascale.org/bdec/}}, the 
Exascale10 workgroup\,\cite{brinkmann14}, the Parallel Data Storage Workshop (PDSW) and the HEC FSIO workshop \cite{bancroft2009}.

There are many I/O workloads studies and performance analysis reports for parallel I/O available.
Additionally, many surveys of enterprise technology usage include predictions of analysis for future storage technology and the storage market such as\,\cite{idc1}.
However, analysis conducted for HPC typically focuses on applications and not on the data center perspective.
Information about data center operational aspects is usually described in file system specific user groups and meetings or described partially in research papers as part of the evaluation environment.

In this workshop, we bring together I/O experts from data centers and application workflows to share current practices for scientific workflows, issues and obstacles for both hardware and the software stack, and R\&D to overcome these issues. 

\section{Organization of the Workshop}

\noindent The workshop content was built on three tracks:
\begin{itemize}
  \item \textbf{Research paper presentations} -- authors needed to submit a paper regarding relevant research for I/O in the datacenter.
  \item \textbf{Talks from I/O experts} -- authors needed to submit a rough outline for the talk related to the operational aspects of the data center.
  \item \textbf{Invited track} for a keynote and two moderated discussion slots.
\end{itemize}        
    
\noindent The CFP has been issued beginning of January.
Important deadlines were:
\begin{itemize}
  \item Submission deadline: 28-02-2016 AoE
  \item Author notification: 23-03-2016
  \item Workshop: 23-05-2016
  \item Camera-ready papers: 23-06-2016  
\end{itemize}

From all submissions, the programm committee selected four talks from I/O experts and four research papers for presentation during the workshop.

\subsection{Programm Committee}
\begin{itemize}
  \item Wolfgang Frings (\textit{Jülich Supercomputing Center, Germany})
  \item Javier Garcia Blas (\textit{University Carlos III of Madrid, Spain})
  \item  Rob Ross (\textit{Argonne National Laboratory, USA})
  \item   Carlos Maltzahn (\textit{University of California, Santa Cruz, USA})
  \item  Kathryn Mohror (\textit{Lawrence Livermore National Laboratory, USA})
  \item  Xiaosong Ma (\textit{North Carolina State University, Oak Ridge National Laboratory, USA})
  \item  Julian Kunkel (\textit{DKRZ, Germany})
  \item  Jay Lofstead (\textit{Sandia National Laboratory, USA})
  \item  Colin McMurtrie (\textit{CSCS, Switzerland})
\end{itemize}



\section{Workshop Summary}

Presentations are uploaded to the webpage: \url{http://wr.informatik.uni-hamburg.de/events/2016/iodc}.
\label{sec:summary}


\bibliographystyle{splncs}
\bibliography{literature}


\end{document}
