\documentclass{llncs}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[official]{eurosym}
\usepackage{graphicx}
\usepackage{color}

\usepackage[hidelinks]{hyperref}
\usepackage[capitalise,noabbrev]{cleveref}


\begin{document}
\mainmatter


\author{Julian M. Kunkel, Jay Lofstead
}

\title{HPC I/O in the Data Center Workshop (HPC-IODC)}

\institute{
	University of Reading \\  Whiteknights \\ Reading RG6 6AY, UK\\ \email{j.m.kunkel@reading.ac.uk}
	\and
	Center for Computing Research\\ Sandia National Laboratories\\ Albuquerque, USA
}

\maketitle{}

% -----------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}

Many public and privately funded data centers host supercomputers for running large scale simulations and analyzing experimental and observational data.
These supercomputers run usually tightly coupled parallel applications that require hardware components that deliver the best performance.
In contrast, commercial data centers, such as Facebook and Google, execute loosely coupled workloads with a broad assumption of regular failures.
The dimension of the data centers is enormous.
A 2013 article summarizes commercial data centers' dimensions\,\cite{data13}.
It estimates, for example, that Facebook hosts around 100\,PB of storage and Google and Microsoft manage around 1 million servers each -- although the hardware is split among several physical data centers -- a modus operandi not suitable for HPC centers.
With the hunger for information, the globally installed storage capacity increases exponentially and is expected to hit 7,235 Exabytes by 2017\,\cite{EXA13}.
This trend is visible in the sales reports of companies such as the disk drive manufacturer Seagate. Within 5 years, they shipped 1 billion HDDs, which means 700.000 units every day\,\cite{SG14}.
With state-of-the-art 8\,TB disks, this would already account for 5.5 exabyte of capacity by day.

Management of the huge amount of data is vital for effective use of the contained information. However, with limited budgets, it is a daunting task for data center operators,
especially as design and storage system required hardware depends heavily on the executed workloads.
A co-factor of the increasing difficulty is the increase in complexity of the storage hierarchy with the adoption of SSD and memory class storage technology.
The US Department of Energy recognizes the importance of data management, listing it among the top 10 research challenges for Exascale\,\cite{top14}. % the document says: “Affordability of data management, both procurement and operational, is a huge challenge.“

There are several initiatives, consortia and special tracks in conferences that target RD\&E audiences.
Examples are the Storage Networking Industry Association (SNIA) for enterprises, the
Big Data and Extreme-Scale Computing (BDEC) initiative\footnote{\url{http://www.exascale.org/bdec/}}, the
Exascale10 workgroup\,\cite{brinkmann14}, the Parallel Data Storage Workshop/Data Intensive Scalable Computing Systems
(PDSW-DISCS) and the HEC FSIO workshop \cite{bancroft2009}.

There are many I/O workloads studies and performance analysis reports for parallel I/O available.
Additionally, many surveys of enterprise technology usage include predictions of analysis for future storage technology and the storage market such as\,\cite{idc1}.
However, analysis conducted for HPC typically focuses on applications and not on the data center perspective.
Information about data center operational aspects is usually described in file system specific user groups and meetings or described partially in research papers as part of the evaluation environment.

In this workshop, we bring together I/O experts from data centers and application workflows to share current practices for scientific workflows, issues and obstacles for both hardware and the software stack, and R\&D to overcome these issues.
This year, we worked closely together with the \textit{Workshop on Performance and Scalability of Storage Systems (WOPSSS)} in respect to the overall agenda organization and planned a joint morning session and split into two separate workshops in the afternoon.


\section{Organization of the Workshop}

The workshop was organized by

\begin{itemize}
	\item Julian Kunkel (\textit{University of Reading, UK})
	\item Jay Lofstead (\textit{Sandia National Lab, USA})
\end{itemize}

\noindent The workshop is supported by the Centre of Excellence in Simulation of Weather and Climate in Europe (ESiWACE) and the Virtual Institute for I/O (VI4IO)\footnote{\url{http://vi4io.org}}.

\noindent The workshop covered three tracks:
\begin{itemize}
  \item \textbf{Research paper presentations} -- authors needed to submit a paper regarding relevant research for I/O in the datacenter.
  \item \textbf{Talks from I/O experts} -- authors needed to submit a rough outline for the talk related to the operational aspects of the data center.
  \item A moderated \textbf{discussion} to identify key issues and potential solutions in the community.
\end{itemize}

\noindent The CFP has been issued beginning of January.
Important deadlines were:
\begin{itemize}
  \item Submission deadline: 2018-04-19  AoE
  \item Author notification: 2018-05-04
  \item Workshop: 2018-06-28
  \item Camera-ready papers: 2018-07-28
\end{itemize}

From all submissions, the program committee selected four talks from I/O experts and four research papers for presentation during the workshop.

\subsection{Programm Committee}
\begin{itemize}
\item Adrian Jackson (\textit{The University of Edinburgh})
\item Ann Gentile (\textit{Sandia National Laboratories})
\item Bing Xie (\textit{Oak Ridge National Lab})
\item Brad Settleyer (\textit{Los Alamos National Laboratory})
\item Feiyi Wang (\textit{Oak Ridge National Lab})
\item George Markomanolis (\textit{King Abdullah University of Science and Technology})
\item Javier Garcia Blas (\textit{University Carlos III of Madrid})
\item Jay Lofstead (\textit{Sandia National Lab})
\item Jean-Thomas Acquaviva (\textit{DDN})
\item Jim Brandt (\textit{Sandia National Laboratories})
\item Julian Kunkel (\textit{DKRZ})
\item Matt Bryson (\textit{University of California, Santa Cruz})
\item Michael Kluge (\textit{TU Dresden})
\item Rob Ross (\textit{Argonne National Laboratory})
\item Sandro Fiore (\textit{CMCC})
\item Suren Byna (\textit{Lawrence Berkeley National Laboratory})
\item Sven Breuner (\textit{ThinkparQ})
\item Thomas Boenisch (\textit{HLRS})
\item Tiago Quintino (\textit{ECMWF})
\item Wolfgang Frings (\textit{Jülich Supercomputing Center})
\end{itemize}



\section{Workshop Summary}
\label{sec:summary}

Throughout the day, on average more than 40 participants attended the workshop.
We had a good mix of talks from I/O experts, data center relevant research and  two discussion sessions.
A short summary of the presentations is given in the following.
The slides of the presentations are available on the workshop's webpage:
\url{https://hps.vi4io.org/events/2018/iodc}.

After a joint welcome message of HPC-IODC with WOPSSS, an invited keynote was given by Phil Carns (Argonne National Laboratory).
In his talk "Understanding and Tuning HPC I/O", the OODA (Observe, Orient, Decide, and Act) loop was used to structure and illustrate challenges and tools in respect to I/O monitoring and analysis.


\subsection{Research Papers}

We received 10 research paper submissions, of which we accepted three papers for presentation and publication.
Additionally, since analyzing and understanding I/O behavior and achieving consistent performance is still the top priority for researchers and data centers, we invited a community paper about Tools for Analyzing I/O.

The research session covered these papers:
\begin{itemize}
\item \textbf{I/O Interference Alleviation on Parallel File Systems Using Server-Side QoS-Based Load-Balancing} by Yuichi Tsujita, Yoshitaka Furutani, Hajime Hida, Keiji Yamamoto, Atsuya Uno, and Fumichika Sueyasu.
Yuichi Tsujita gave an overview of the K computer and described the data staging methods to improve I/O performance.
A slow metadata performance during the staging procedure was investigated by analyzing various statistics from the metadata server.
To improve the behavior of the system, a quality-of-service using fair-share was introduced at the server and analyzed.


\item \textbf{Analyzing the I/O scalability of a Particle-in-Cell parallel code} by
Sandra Mendez, Nicolay Hammer, and Anupam Karmakar.
In the talk, Sandra described the systems at LRZ, the PiC code for which the performance analysis study was made.
The authors modeled exhibited application behavior mathematically to understand the relation between assessed performance and application activity better.

\item \textbf{Cost and Performance Modeling for Earth System Data Management and Beyond} by Jakob Luettgau and Julian Kunkel.
In the talk, example graph based and tabular models for communicating and visualizing system characteristics were introduced.
The notion of the talk was to identify possibilities to standardize such visualizations and discuss the potential benefits.
\end{itemize}

\subsection{Talks from Experts}

The following talks from experts included some basic information about the site and typical application profiles but focuses on information regarding I/O tools and strategies applied to mitigate the pressing issues.

\begin{itemize}
\item First, Lionel Vincent introduced a strategy from Bull for \textbf{Self-Optimized Strategy for IO Accelerator Parametrization}.
In this approach, the system learns to set tuning parameters that are unspecified by users.
A gradient-free optimization method is used to determine the next candidate value for a parameter.

\item Next, Patrick Widener talked about \textbf{Addressing data center storage diversity for high-performance computing applications using Faodel}.
As data management service, it acts as a middleware between asynchronous or bulk-sychronous applications, resource manager, analysis, and data caching.
The design and architecture is described together with use-cases.

\item Then, Glenn Lockwood spoke about \textbf{Planning for the Future of Storage for HPC: 2020, 2025, and Beyond} for NERSC.
Firstly, the current and planned infrastructure is introduced.
An overall roadmap of NERSC is described discussing issues with the current and future storage hierarchy alongside with technological trends.


\item Next, Stephen Simms described a collaboration with DDN for  \textbf{Lustre-On-ZFS}.
After praising the benefits of ZFS a performance study is conducted.
It turned out, that LDISKFS has compared to ZFS still a performance advantage in terms of the metadata operations getattr and setattr.


\item Next, Simon Smart described the \textbf{Development of a high-performance distributed object-store for Numerical Weather Prediction and Climate model data}.
After describing the ECMWF workflow, the MARS database is introduced that allows scientists to address data using scientific metadata from a fixed catalog.
Additional insight about the usage of NVM is provided.

\item Finally, Juan R. Herrera introduced approaches for \textbf{Exploiting Nonvolatile memory for HPC}. The talk illustrated the NextGenIO project which utilizing new hardware technology develops tools for analyzing I/O.
\end{itemize}


\subsection{Discussion Sessions}

The major distinguishing feature for this workshop compared to other venues is the discussion rounds.
The opportunity for themed, open discussions about issues both pressing and relevant to the data center
community facilitates sharing experiences, solutions, and problems.

In a first discussion, we focused on the possible community development of next generation semantic interfaces. Julian Kunkel provoked the community with a presentation about a possible approach that follows the successful approach of MPI which lead to a discussion how such a goal could be achieved.
The community page is launched and supported by VI4IO, see \url{https://ngi.vi4io.org}.


In the announced morning discussion, various topics have been briefly discussed.


In the afternoon discussion, ...
IO-500 benchmark, see \url{http://io500.org}.
... Training ...




\bibliographystyle{splncs03}
\bibliography{literature}{}

\end{document}
